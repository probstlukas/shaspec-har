{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sha Enc input shape:  (1, 1, 128, 54)\n",
      "---------------- Specific Encoder ----------------\n",
      "--------------------------------\n",
      "--> B x F x T x C\n",
      "Before applying conv layers:  torch.Size([1, 1, 128, 18])\n",
      "After applying conv layer 1:  torch.Size([1, 64, 62, 18])\n",
      "After applying conv layer 2:  torch.Size([1, 64, 29, 18])\n",
      "After applying conv layer 3:  torch.Size([1, 64, 13, 18])\n",
      "After applying conv layers:  torch.Size([1, 64, 5, 18])\n",
      "--> B x F x T* x C\n",
      "size  torch.Size([1, 64, 18, 1])\n",
      "torch.Size([1, 64, 18, 1])\n",
      "size  torch.Size([1, 64, 18, 1])\n",
      "torch.Size([1, 64, 18, 1])\n",
      "size  torch.Size([1, 64, 18, 1])\n",
      "torch.Size([1, 64, 18, 1])\n",
      "size  torch.Size([1, 64, 18, 1])\n",
      "torch.Size([1, 64, 18, 1])\n",
      "size  torch.Size([1, 64, 18, 1])\n",
      "torch.Size([1, 64, 18, 1])\n",
      "After applying self-attention:  torch.Size([1, 64, 18, 5])\n",
      "After permuting:  torch.Size([1, 18, 64, 5])\n",
      "After reshaping:  torch.Size([1, 18, 320])\n",
      "After passing through fc layer:  torch.Size([1, 18, 128])\n",
      "--------------------------------\n",
      "--> B x F x T x C\n",
      "Before applying conv layers:  torch.Size([1, 1, 128, 18])\n",
      "After applying conv layer 1:  torch.Size([1, 64, 62, 18])\n",
      "After applying conv layer 2:  torch.Size([1, 64, 29, 18])\n",
      "After applying conv layer 3:  torch.Size([1, 64, 13, 18])\n",
      "After applying conv layers:  torch.Size([1, 64, 5, 18])\n",
      "--> B x F x T* x C\n",
      "size  torch.Size([1, 64, 18, 1])\n",
      "torch.Size([1, 64, 18, 1])\n",
      "size  torch.Size([1, 64, 18, 1])\n",
      "torch.Size([1, 64, 18, 1])\n",
      "size  torch.Size([1, 64, 18, 1])\n",
      "torch.Size([1, 64, 18, 1])\n",
      "size  torch.Size([1, 64, 18, 1])\n",
      "torch.Size([1, 64, 18, 1])\n",
      "size  torch.Size([1, 64, 18, 1])\n",
      "torch.Size([1, 64, 18, 1])\n",
      "After applying self-attention:  torch.Size([1, 64, 18, 5])\n",
      "After permuting:  torch.Size([1, 18, 64, 5])\n",
      "After reshaping:  torch.Size([1, 18, 320])\n",
      "After passing through fc layer:  torch.Size([1, 18, 128])\n",
      "--------------------------------\n",
      "--> B x F x T x C\n",
      "Before applying conv layers:  torch.Size([1, 1, 128, 18])\n",
      "After applying conv layer 1:  torch.Size([1, 64, 62, 18])\n",
      "After applying conv layer 2:  torch.Size([1, 64, 29, 18])\n",
      "After applying conv layer 3:  torch.Size([1, 64, 13, 18])\n",
      "After applying conv layers:  torch.Size([1, 64, 5, 18])\n",
      "--> B x F x T* x C\n",
      "size  torch.Size([1, 64, 18, 1])\n",
      "torch.Size([1, 64, 18, 1])\n",
      "size  torch.Size([1, 64, 18, 1])\n",
      "torch.Size([1, 64, 18, 1])\n",
      "size  torch.Size([1, 64, 18, 1])\n",
      "torch.Size([1, 64, 18, 1])\n",
      "size  torch.Size([1, 64, 18, 1])\n",
      "torch.Size([1, 64, 18, 1])\n",
      "size  torch.Size([1, 64, 18, 1])\n",
      "torch.Size([1, 64, 18, 1])\n",
      "After applying self-attention:  torch.Size([1, 64, 18, 5])\n",
      "After permuting:  torch.Size([1, 18, 64, 5])\n",
      "After reshaping:  torch.Size([1, 18, 320])\n",
      "After passing through fc layer:  torch.Size([1, 18, 128])\n",
      "SPECIFIC FEATURES:  [tensor([[[ 0.0406, -0.0452, -0.0050,  ...,  0.0694, -0.0436,  0.0142],\n",
      "         [ 0.0361, -0.0371,  0.0039,  ...,  0.0855, -0.0344,  0.0262],\n",
      "         [ 0.0306, -0.0047,  0.0037,  ...,  0.0597, -0.0406,  0.0286],\n",
      "         ...,\n",
      "         [ 0.0261, -0.0406,  0.0103,  ...,  0.0591, -0.0495,  0.0265],\n",
      "         [ 0.0328, -0.0386,  0.0081,  ...,  0.0511, -0.0424,  0.0131],\n",
      "         [ 0.0205, -0.0299,  0.0286,  ...,  0.0632, -0.0480,  0.0204]]],\n",
      "       grad_fn=<ViewBackward0>), tensor([[[ 0.0214, -0.0208, -0.0240,  ..., -0.0276,  0.0102, -0.0400],\n",
      "         [ 0.0082, -0.0324, -0.0166,  ..., -0.0091,  0.0065, -0.0465],\n",
      "         [ 0.0255, -0.0045, -0.0248,  ..., -0.0121,  0.0392, -0.0325],\n",
      "         ...,\n",
      "         [ 0.0121, -0.0189, -0.0215,  ..., -0.0129,  0.0375, -0.0240],\n",
      "         [ 0.0237,  0.0041, -0.0447,  ..., -0.0250,  0.0379, -0.0367],\n",
      "         [ 0.0122, -0.0237, -0.0217,  ..., -0.0278,  0.0271, -0.0351]]],\n",
      "       grad_fn=<ViewBackward0>), tensor([[[-0.0389,  0.0022,  0.0194,  ...,  0.0194,  0.0596,  0.0333],\n",
      "         [-0.0245, -0.0192, -0.0016,  ...,  0.0231,  0.0575,  0.0255],\n",
      "         [-0.0456, -0.0062,  0.0180,  ...,  0.0121,  0.0567,  0.0193],\n",
      "         ...,\n",
      "         [-0.0423, -0.0071,  0.0062,  ...,  0.0173,  0.0675,  0.0147],\n",
      "         [-0.0336, -0.0071,  0.0230,  ...,  0.0140,  0.0711,  0.0207],\n",
      "         [-0.0397, -0.0119,  0.0289,  ...,  0.0119,  0.0663,  0.0422]]],\n",
      "       grad_fn=<ViewBackward0>)]\n",
      "---------------- Shared Encoder ----------------\n",
      "torch.Size([1, 1, 128, 54])\n",
      "torch.Size([1, 64, 62, 54])\n",
      "torch.Size([1, 64, 29, 54])\n",
      "torch.Size([1, 64, 13, 54])\n",
      "torch.Size([1, 64, 5, 54])\n",
      "size  torch.Size([1, 64, 54, 1])\n",
      "torch.Size([1, 64, 54, 1])\n",
      "size  torch.Size([1, 64, 54, 1])\n",
      "torch.Size([1, 64, 54, 1])\n",
      "size  torch.Size([1, 64, 54, 1])\n",
      "torch.Size([1, 64, 54, 1])\n",
      "size  torch.Size([1, 64, 54, 1])\n",
      "torch.Size([1, 64, 54, 1])\n",
      "size  torch.Size([1, 64, 54, 1])\n",
      "torch.Size([1, 64, 54, 1])\n",
      "After refined: torch.Size([1, 64, 54, 5])\n",
      "Before splitting:  torch.Size([1, 54, 128])\n",
      "After splitting:0  torch.Size([1, 18, 128])\n",
      "After splitting:1  torch.Size([1, 18, 128])\n",
      "After splitting:2  torch.Size([1, 18, 128])\n",
      "SHARED FEATURES:  (tensor([[[ 0.0231,  0.0090,  0.0721,  ..., -0.0257,  0.0021, -0.0365],\n",
      "         [ 0.0350,  0.0008,  0.0646,  ..., -0.0042,  0.0014, -0.0466],\n",
      "         [ 0.0346,  0.0103,  0.0757,  ..., -0.0084,  0.0016, -0.0268],\n",
      "         ...,\n",
      "         [ 0.0169, -0.0036,  0.0843,  ...,  0.0025,  0.0068, -0.0369],\n",
      "         [ 0.0413, -0.0067,  0.1087,  ..., -0.0137, -0.0117, -0.0354],\n",
      "         [ 0.0274, -0.0192,  0.0863,  ...,  0.0022,  0.0037, -0.0346]]],\n",
      "       grad_fn=<SplitBackward0>), tensor([[[ 0.0286,  0.0047,  0.0814,  ..., -0.0084, -0.0041, -0.0515],\n",
      "         [ 0.0148, -0.0151,  0.0878,  ...,  0.0053,  0.0062, -0.0420],\n",
      "         [ 0.0403,  0.0135,  0.0856,  ..., -0.0034, -0.0074, -0.0419],\n",
      "         ...,\n",
      "         [ 0.0401, -0.0197,  0.0811,  ...,  0.0061, -0.0030, -0.0387],\n",
      "         [ 0.0368,  0.0055,  0.0713,  ...,  0.0076,  0.0120, -0.0576],\n",
      "         [ 0.0313, -0.0018,  0.0606,  ..., -0.0115,  0.0103, -0.0304]]],\n",
      "       grad_fn=<SplitBackward0>), tensor([[[ 0.0384,  0.0141,  0.0587,  ...,  0.0077, -0.0092, -0.0240],\n",
      "         [ 0.0402, -0.0026,  0.0787,  ..., -0.0020, -0.0155, -0.0319],\n",
      "         [ 0.0301, -0.0094,  0.0727,  ...,  0.0026,  0.0040, -0.0394],\n",
      "         ...,\n",
      "         [ 0.0332, -0.0017,  0.0885,  ..., -0.0087,  0.0048, -0.0350],\n",
      "         [ 0.0512, -0.0217,  0.0679,  ..., -0.0038, -0.0029, -0.0342],\n",
      "         [ 0.0399, -0.0012,  0.0758,  ..., -0.0024,  0.0083, -0.0369]]],\n",
      "       grad_fn=<SplitBackward0>))\n",
      "---------------- Putting things together ----------------\n",
      "CONCATENATED:  torch.Size([1, 18, 256])\n",
      "PROJECTED:  torch.Size([1, 18, 128])\n",
      "--------------------------------\n",
      "torch.Size([1, 18, 128])\n",
      "torch.Size([1, 18, 128])\n",
      "MODALITY EMBEDDING:  torch.Size([1, 18, 128])\n",
      "CONCATENATED:  torch.Size([1, 18, 256])\n",
      "PROJECTED:  torch.Size([1, 18, 128])\n",
      "--------------------------------\n",
      "torch.Size([1, 18, 128])\n",
      "torch.Size([1, 18, 128])\n",
      "MODALITY EMBEDDING:  torch.Size([1, 18, 128])\n",
      "CONCATENATED:  torch.Size([1, 18, 256])\n",
      "PROJECTED:  torch.Size([1, 18, 128])\n",
      "--------------------------------\n",
      "torch.Size([1, 18, 128])\n",
      "torch.Size([1, 18, 128])\n",
      "MODALITY EMBEDDING:  torch.Size([1, 18, 128])\n",
      "3\n",
      "torch.Size([1, 18, 384])\n"
     ]
    }
   ],
   "source": [
    "from models.ShaSpec import SpecificEncoder, SharedEncoder, ShaSpec\n",
    "import torch\n",
    "\n",
    "# Number of modalities\n",
    "modalities_num = 3  # For example, 3 modalities\n",
    "\n",
    "# Dummy input for each modality\n",
    "# B F T C\n",
    "input_shape = (1, 1, 128, 18)  \n",
    "dummy_inputs = [torch.randn(input_shape) for _ in range(modalities_num)]\n",
    "\n",
    "# Create a ShaSpec instance\n",
    "# You need to define the number of classes for the output\n",
    "number_class = 10  # Example: 10 different classes\n",
    "shaspec_model = ShaSpec(input_shape, number_class, modalities_num)\n",
    "\n",
    "# Forward pass with the dummy inputs\n",
    "output = shaspec_model(dummy_inputs)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
