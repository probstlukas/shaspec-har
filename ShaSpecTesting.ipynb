{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sha Enc input shape:  (1, 1, 128, 54)\n",
      "---------------- Specific Encoder ----------------\n",
      "--------------------------------\n",
      "--> B x F x T x C\n",
      "Before applying conv layers:  torch.Size([1, 1, 128, 18])\n",
      "After applying conv layer 1:  torch.Size([1, 64, 62, 18])\n",
      "After applying conv layer 2:  torch.Size([1, 64, 29, 18])\n",
      "After applying conv layer 3:  torch.Size([1, 64, 13, 18])\n",
      "After applying conv layers:  torch.Size([1, 64, 5, 18])\n",
      "--> B x F' x T* x C\n",
      "size  torch.Size([1, 64, 18, 1])\n",
      "torch.Size([1, 64, 18, 1])\n",
      "size  torch.Size([1, 64, 18, 1])\n",
      "torch.Size([1, 64, 18, 1])\n",
      "size  torch.Size([1, 64, 18, 1])\n",
      "torch.Size([1, 64, 18, 1])\n",
      "size  torch.Size([1, 64, 18, 1])\n",
      "torch.Size([1, 64, 18, 1])\n",
      "size  torch.Size([1, 64, 18, 1])\n",
      "torch.Size([1, 64, 18, 1])\n",
      "After applying self-attention:  torch.Size([1, 64, 18, 5])\n",
      "After permuting:  torch.Size([1, 18, 64, 5])\n",
      "After reshaping:  torch.Size([1, 18, 320])\n",
      "After passing through fc layer:  torch.Size([1, 18, 128])\n",
      "--------------------------------\n",
      "--> B x F x T x C\n",
      "Before applying conv layers:  torch.Size([1, 1, 128, 18])\n",
      "After applying conv layer 1:  torch.Size([1, 64, 62, 18])\n",
      "After applying conv layer 2:  torch.Size([1, 64, 29, 18])\n",
      "After applying conv layer 3:  torch.Size([1, 64, 13, 18])\n",
      "After applying conv layers:  torch.Size([1, 64, 5, 18])\n",
      "--> B x F' x T* x C\n",
      "size  torch.Size([1, 64, 18, 1])\n",
      "torch.Size([1, 64, 18, 1])\n",
      "size  torch.Size([1, 64, 18, 1])\n",
      "torch.Size([1, 64, 18, 1])\n",
      "size  torch.Size([1, 64, 18, 1])\n",
      "torch.Size([1, 64, 18, 1])\n",
      "size  torch.Size([1, 64, 18, 1])\n",
      "torch.Size([1, 64, 18, 1])\n",
      "size  torch.Size([1, 64, 18, 1])\n",
      "torch.Size([1, 64, 18, 1])\n",
      "After applying self-attention:  torch.Size([1, 64, 18, 5])\n",
      "After permuting:  torch.Size([1, 18, 64, 5])\n",
      "After reshaping:  torch.Size([1, 18, 320])\n",
      "After passing through fc layer:  torch.Size([1, 18, 128])\n",
      "--------------------------------\n",
      "--> B x F x T x C\n",
      "Before applying conv layers:  torch.Size([1, 1, 128, 18])\n",
      "After applying conv layer 1:  torch.Size([1, 64, 62, 18])\n",
      "After applying conv layer 2:  torch.Size([1, 64, 29, 18])\n",
      "After applying conv layer 3:  torch.Size([1, 64, 13, 18])\n",
      "After applying conv layers:  torch.Size([1, 64, 5, 18])\n",
      "--> B x F' x T* x C\n",
      "size  torch.Size([1, 64, 18, 1])\n",
      "torch.Size([1, 64, 18, 1])\n",
      "size  torch.Size([1, 64, 18, 1])\n",
      "torch.Size([1, 64, 18, 1])\n",
      "size  torch.Size([1, 64, 18, 1])\n",
      "torch.Size([1, 64, 18, 1])\n",
      "size  torch.Size([1, 64, 18, 1])\n",
      "torch.Size([1, 64, 18, 1])\n",
      "size  torch.Size([1, 64, 18, 1])\n",
      "torch.Size([1, 64, 18, 1])\n",
      "After applying self-attention:  torch.Size([1, 64, 18, 5])\n",
      "After permuting:  torch.Size([1, 18, 64, 5])\n",
      "After reshaping:  torch.Size([1, 18, 320])\n",
      "After passing through fc layer:  torch.Size([1, 18, 128])\n",
      "SPECIFIC FEATURES:  [tensor([[[-0.0055,  0.0118, -0.0121,  ..., -0.0358,  0.0510,  0.0162],\n",
      "         [-0.0229,  0.0253,  0.0149,  ..., -0.0184,  0.0488,  0.0176],\n",
      "         [-0.0165, -0.0047,  0.0016,  ..., -0.0137,  0.0592,  0.0107],\n",
      "         ...,\n",
      "         [-0.0354,  0.0033,  0.0093,  ..., -0.0286,  0.0677,  0.0169],\n",
      "         [-0.0129,  0.0232, -0.0028,  ..., -0.0166,  0.0569,  0.0210],\n",
      "         [-0.0160, -0.0035,  0.0031,  ..., -0.0164,  0.0319,  0.0034]]],\n",
      "       grad_fn=<ViewBackward0>), tensor([[[-0.0232, -0.0620,  0.0420,  ...,  0.0179, -0.0181, -0.0377],\n",
      "         [-0.0355, -0.0824,  0.0325,  ...,  0.0148, -0.0105, -0.0168],\n",
      "         [-0.0261, -0.0961,  0.0633,  ...,  0.0032, -0.0068, -0.0309],\n",
      "         ...,\n",
      "         [-0.0412, -0.0731,  0.0397,  ...,  0.0215, -0.0047, -0.0201],\n",
      "         [-0.0329, -0.0862,  0.0346,  ...,  0.0060, -0.0119, -0.0342],\n",
      "         [-0.0596, -0.0780,  0.0335,  ...,  0.0187,  0.0130, -0.0082]]],\n",
      "       grad_fn=<ViewBackward0>), tensor([[[ 0.0229,  0.0333,  0.0415,  ..., -0.0287,  0.0270, -0.0533],\n",
      "         [ 0.0109,  0.0372,  0.0335,  ..., -0.0293,  0.0242, -0.0276],\n",
      "         [ 0.0215,  0.0242,  0.0512,  ..., -0.0256,  0.0329, -0.0498],\n",
      "         ...,\n",
      "         [ 0.0258,  0.0155,  0.0485,  ..., -0.0072,  0.0171, -0.0378],\n",
      "         [ 0.0337,  0.0302,  0.0386,  ..., -0.0264,  0.0410, -0.0361],\n",
      "         [ 0.0279,  0.0520,  0.0544,  ..., -0.0208,  0.0257, -0.0451]]],\n",
      "       grad_fn=<ViewBackward0>)]\n",
      "---------------- Shared Encoder ----------------\n",
      "torch.Size([1, 1, 128, 54])\n",
      "torch.Size([1, 64, 62, 54])\n",
      "torch.Size([1, 64, 29, 54])\n",
      "torch.Size([1, 64, 13, 54])\n",
      "torch.Size([1, 64, 5, 54])\n",
      "size  torch.Size([1, 64, 54, 1])\n",
      "torch.Size([1, 64, 54, 1])\n",
      "size  torch.Size([1, 64, 54, 1])\n",
      "torch.Size([1, 64, 54, 1])\n",
      "size  torch.Size([1, 64, 54, 1])\n",
      "torch.Size([1, 64, 54, 1])\n",
      "size  torch.Size([1, 64, 54, 1])\n",
      "torch.Size([1, 64, 54, 1])\n",
      "size  torch.Size([1, 64, 54, 1])\n",
      "torch.Size([1, 64, 54, 1])\n",
      "After refined: torch.Size([1, 64, 54, 5])\n",
      "Before splitting:  torch.Size([1, 54, 128])\n",
      "After splitting:0  torch.Size([1, 18, 128])\n",
      "After splitting:1  torch.Size([1, 18, 128])\n",
      "After splitting:2  torch.Size([1, 18, 128])\n",
      "SHARED FEATURES:  (tensor([[[-0.0119, -0.0461, -0.0416,  ...,  0.0292, -0.0122, -0.0182],\n",
      "         [ 0.0062, -0.0469, -0.0344,  ...,  0.0182, -0.0147, -0.0310],\n",
      "         [ 0.0262, -0.0436, -0.0200,  ...,  0.0390, -0.0191, -0.0231],\n",
      "         ...,\n",
      "         [ 0.0198, -0.0747, -0.0218,  ...,  0.0323,  0.0075, -0.0370],\n",
      "         [ 0.0069, -0.0440, -0.0315,  ...,  0.0372, -0.0136, -0.0326],\n",
      "         [ 0.0010, -0.0451, -0.0265,  ...,  0.0277,  0.0049, -0.0267]]],\n",
      "       grad_fn=<SplitBackward0>), tensor([[[-0.0091, -0.0495, -0.0128,  ...,  0.0141, -0.0011, -0.0417],\n",
      "         [ 0.0239, -0.0490, -0.0334,  ...,  0.0386, -0.0147, -0.0291],\n",
      "         [ 0.0060, -0.0608, -0.0155,  ...,  0.0135, -0.0123, -0.0497],\n",
      "         ...,\n",
      "         [ 0.0053, -0.0385, -0.0158,  ...,  0.0192, -0.0076, -0.0309],\n",
      "         [ 0.0151, -0.0408, -0.0050,  ...,  0.0239, -0.0077, -0.0228],\n",
      "         [-0.0150, -0.0478, -0.0197,  ...,  0.0239, -0.0088, -0.0202]]],\n",
      "       grad_fn=<SplitBackward0>), tensor([[[ 0.0076, -0.0404, -0.0412,  ...,  0.0144, -0.0138, -0.0320],\n",
      "         [ 0.0206, -0.0469, -0.0238,  ...,  0.0355,  0.0007, -0.0332],\n",
      "         [-0.0062, -0.0427, -0.0169,  ...,  0.0338, -0.0069, -0.0378],\n",
      "         ...,\n",
      "         [-0.0109, -0.0527, -0.0210,  ...,  0.0265, -0.0111, -0.0381],\n",
      "         [ 0.0003, -0.0561, -0.0260,  ...,  0.0303,  0.0142, -0.0308],\n",
      "         [ 0.0054, -0.0647, -0.0042,  ...,  0.0199,  0.0043, -0.0383]]],\n",
      "       grad_fn=<SplitBackward0>))\n",
      "CONCATENATED:  torch.Size([1, 18, 256])\n",
      "PROJECTED:  torch.Size([1, 18, 128])\n",
      "--------------------------------\n",
      "torch.Size([1, 18, 128])\n",
      "torch.Size([1, 18, 128])\n",
      "MODALITY EMBEDDING:  torch.Size([1, 18, 128])\n",
      "CONCATENATED:  torch.Size([1, 18, 256])\n",
      "PROJECTED:  torch.Size([1, 18, 128])\n",
      "--------------------------------\n",
      "torch.Size([1, 18, 128])\n",
      "torch.Size([1, 18, 128])\n",
      "MODALITY EMBEDDING:  torch.Size([1, 18, 128])\n",
      "CONCATENATED:  torch.Size([1, 18, 256])\n",
      "PROJECTED:  torch.Size([1, 18, 128])\n",
      "--------------------------------\n",
      "torch.Size([1, 18, 128])\n",
      "torch.Size([1, 18, 128])\n",
      "MODALITY EMBEDDING:  torch.Size([1, 18, 128])\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "cat() received an invalid combination of arguments - got (NoneType, dim=int), but expected one of:\n * (tuple of Tensors tensors, int dim, *, Tensor out)\n * (tuple of Tensors tensors, name dim, *, Tensor out)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 18\u001b[0m\n\u001b[1;32m     15\u001b[0m shaspec_model \u001b[38;5;241m=\u001b[39m ShaSpec(input_shape, number_class, modalities_num)\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# Forward pass with the dummy inputs\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mshaspec_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdummy_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Bachelorarbeit/I2S0W2C2_CFC/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Bachelorarbeit/I2S0W2C2_CFC/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/Bachelorarbeit/I2S0W2C2_CFC/models/ShaSpec.py:356\u001b[0m, in \u001b[0;36mShaSpec.forward\u001b[0;34m(self, x_list)\u001b[0m\n\u001b[1;32m    353\u001b[0m modality_embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresidual_block(specific_features, shared_features)\n\u001b[1;32m    355\u001b[0m \u001b[38;5;66;03m# Prepare features for decoder by concatenating them along the F dimension\u001b[39;00m\n\u001b[0;32m--> 356\u001b[0m concatenated_features \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodality_embeddings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    357\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShape of concatenated features: \u001b[39m\u001b[38;5;124m\"\u001b[39m, concatenated_features\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m    359\u001b[0m prediction \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder(concatenated_features)\n",
      "\u001b[0;31mTypeError\u001b[0m: cat() received an invalid combination of arguments - got (NoneType, dim=int), but expected one of:\n * (tuple of Tensors tensors, int dim, *, Tensor out)\n * (tuple of Tensors tensors, name dim, *, Tensor out)\n"
     ]
    }
   ],
   "source": [
    "from models.ShaSpec import SpecificEncoder, SharedEncoder, ShaSpec\n",
    "import torch\n",
    "\n",
    "# Number of modalities\n",
    "modalities_num = 3  # For example, 3 modalities\n",
    "\n",
    "# Dummy input for each modality\n",
    "# B F T C\n",
    "input_shape = (1, 1, 128, 18)  \n",
    "dummy_inputs = [torch.randn(input_shape) for _ in range(modalities_num)]\n",
    "\n",
    "# Create a ShaSpec instance\n",
    "# You need to define the number of classes for the output\n",
    "number_class = 10  # Example: 10 different classes\n",
    "shaspec_model = ShaSpec(input_shape, number_class, modalities_num)\n",
    "\n",
    "# Forward pass with the dummy inputs\n",
    "output = shaspec_model(dummy_inputs)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
