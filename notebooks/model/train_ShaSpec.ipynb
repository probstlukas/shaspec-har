{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../../\")\n",
    "from experiment import Exp\n",
    "\n",
    "from dataloaders import data_set,data_dict\n",
    "import torch\n",
    "import yaml\n",
    "import os\n",
    "\n",
    "from torch.optim import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Parameter for training\n",
    "\n",
    "class dotdict(dict):\n",
    "    \"\"\"dot.notation access to dictionary attributes\"\"\"\n",
    "    __getattr__ = dict.get\n",
    "    __setattr__ = dict.__setitem__\n",
    "    __delattr__ = dict.__delitem__\n",
    "\n",
    "args = dotdict()   \n",
    "\n",
    "args.to_save_path     = \"/home/lukasprobst/Documents/Bachelorarbeit/Run_logs\"           \n",
    "args.freq_save_path   = \"/home/lukasprobst/Documents/Bachelorarbeit/Freq_data\"\n",
    "args.window_save_path = \"/home/lukasprobst/Documents/Bachelorarbeit/Sliding_window\"\n",
    "args.root_path        = \"/home/lukasprobst/Documents/Bachelorarbeit/datasets\"\n",
    "\n",
    "args.drop_transition  = False\n",
    "# Nomalization of the data, depends on dataset!\n",
    "args.datanorm_type    = \"standardization\" # None ,\"standardization\", \"minmax\"\n",
    "\n",
    "args.batch_size       = 64 # Reduce if not enough graphic memory\n",
    "args.shuffle          = True # Give NN more variety to learn\n",
    "args.drop_last        = True # Drop last batch if smaller than batch size\n",
    "args.train_vali_quote = 0.90 # 90% training, 10% validation                     \n",
    "\n",
    "# training setting \n",
    "args.train_epochs            = 60\n",
    "\n",
    "args.learning_rate           = 0.001 # 10^(-3)\n",
    "args.learning_rate_patience  = 20 # Reduce learning rate every 20 epochs\n",
    "args.learning_rate_factor    = 0.1 # Reduce learning rate by 10%\n",
    "\n",
    "args.early_stop_patience     = 15 # Stop after 15 epochs without improvement\n",
    "\n",
    "args.use_gpu                 = True if torch.cuda.is_available() else False\n",
    "args.gpu                     = 0 # Use first GPU\n",
    "args.use_multi_gpu           = False # Use multiple GPUs\n",
    "\n",
    "args.optimizer               = \"Adam\"\n",
    "args.criterion               = \"CrossEntropy\"\n",
    "\n",
    "# From ShaSpec paper:\n",
    "# - 60 epochs\n",
    "# - batch norm and dropout\n",
    "# - adam optimizer with 10^(-2) weight decay\n",
    "# - initial learning rate 10^(-3) and reduced by 10% every 20 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get content of folder args.window_save_path\n",
    "files = os.listdir(args.window_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n"
     ]
    }
   ],
   "source": [
    "### Parameter for data\n",
    "\n",
    "args.seed                             = 2 # Seed for reproducibility\n",
    "\n",
    "args.data_name                        =  \"dsads\"\n",
    "\n",
    "# Wavelet calculates frequence parameters from time series data, don't need that\n",
    "args.wavelet_filtering                = False\n",
    "args.wavelet_filtering_regularization = False\n",
    "args.wavelet_filtering_finetuning     = False\n",
    "args.wavelet_filtering_finetuning_percent = 0.5\n",
    "args.wavelet_filtering_learnable      = False\n",
    "args.wavelet_filtering_layernorm      = False\n",
    "\n",
    "args.regulatization_tradeoff          = 0\n",
    "args.number_wavelet_filtering         = 6\n",
    "\n",
    "# Augmentation difference\n",
    "args.difference          =  False\n",
    "\n",
    "# Augmentation filtering\n",
    "args.filtering           =  False\n",
    "\n",
    "# Augmentation  magnitude\n",
    "args.magnitude           =  False\n",
    "args.weighted_sampler    = False\n",
    "\n",
    "# MIXUP\n",
    "args.mixup_probability  = 0.5\n",
    "args.mixup_alpha  = 0.5\n",
    "\n",
    "# Random Augmentation\n",
    "args.random_augmentation_prob = 0.5\n",
    "args.random_augmentation_config = {\"jitter\":True,\n",
    "                                   \"moving_average\":True,\n",
    "                                   \"magnitude_scaling\":True,\n",
    "                                   \"magnitude_warp\":True,\n",
    "                                   \"magnitude_shift\":True,\n",
    "                                   \"time_warp\":True,\n",
    "                                   \"window_warp\":True,\n",
    "                                   \"window_slice\":True,\n",
    "                                   \"random_sampling\":True,\n",
    "                                   \"slope_adding\":True\n",
    "                                   }\n",
    "random_augmentation_nr = 0\n",
    "for key in args.random_augmentation_config.keys():\n",
    "    if args.random_augmentation_config[key]:\n",
    "        random_augmentation_nr = random_augmentation_nr+1\n",
    "args.random_augmentation_nr = random_augmentation_nr\n",
    "args.max_aug = 3\n",
    "\n",
    "args.pos_select       = None\n",
    "args.sensor_select    = None\n",
    "\n",
    "args.representation_type = \"time\"\n",
    "args.exp_mode            = \"LOCV\"\n",
    "if args.data_name      ==  \"skodar\":\n",
    "    args.exp_mode            = \"SOCV\"\n",
    "    \n",
    "config_file = open('../../configs/data.yaml', mode='r')\n",
    "data_config = yaml.load(config_file, Loader=yaml.FullLoader)\n",
    "config = data_config[args.data_name]\n",
    "\n",
    "args.root_path       = os.path.join(args.root_path,config[\"filename\"])\n",
    "args.sampling_freq   = config[\"sampling_freq\"]\n",
    "args.classes_num     =  config[\"num_classes\"]\n",
    "args.modalities_num  =  config[\"num_modalities\"]\n",
    "window_seconds       = config[\"window_seconds\"]\n",
    "print\n",
    "args.windowsize      =   int(window_seconds * args.sampling_freq) \n",
    "args.input_length    =  args.windowsize\n",
    "# Input information\n",
    "# For ShaSpec we want the number of channels per modality\n",
    "args.c_in            =  config[\"num_channels\"] // args.modalities_num\n",
    "\n",
    "print(args.c_in )\n",
    "if args.difference:\n",
    "    args.c_in  = args.c_in * 2\n",
    "    print(args.c_in )\n",
    "if args.filtering:\n",
    "    for col in config[\"sensors\"]:\n",
    "        if \"acc\" in col:\n",
    "            args.c_in = args.c_in+1\n",
    "            print(args.c_in )\n",
    "\n",
    "if args.wavelet_filtering :\n",
    "    if args.windowsize%2==1:\n",
    "        N_ds = int(torch.log2(torch.tensor(args.windowsize-1)).floor()) - 2\n",
    "    else:\n",
    "        N_ds = int(torch.log2(torch.tensor(args.windowsize)).floor()) - 2\n",
    "\n",
    "    args.f_in            =  args.number_wavelet_filtering*N_ds+1\n",
    "else:\n",
    "    args.f_in            =  1\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args.c_in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args.classes_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use GPU: cuda:0\n",
      "Build the ShaSpec model!\n",
      "Done!\n",
      "Parameter : 366517\n",
      "Set the seed as :  2\n"
     ]
    }
   ],
   "source": [
    "### Parameter for the model\n",
    "\n",
    "args.model_type = \"shaspec\"\n",
    "\n",
    "args.activation = \"ReLU\"\n",
    "args.decoder_type = \"FC\"\n",
    "args.shared_encoder_type = \"concatenated\"\n",
    "\n",
    "exp = Exp(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "model_builder(\n",
       "  (model): ShaSpec(\n",
       "    (specific_encoders): ModuleList(\n",
       "      (0-4): 5 x SpecificEncoder(\n",
       "        (conv_layers): Sequential(\n",
       "          (0): Conv2d(1, 42, kernel_size=(5, 1), stride=(2, 1))\n",
       "          (1): ReLU()\n",
       "          (2): Conv2d(42, 42, kernel_size=(5, 1), stride=(2, 1))\n",
       "          (3): ReLU()\n",
       "          (4): Conv2d(42, 42, kernel_size=(5, 1), stride=(2, 1))\n",
       "          (5): ReLU()\n",
       "          (6): Conv2d(42, 42, kernel_size=(5, 1), stride=(2, 1))\n",
       "          (7): ReLU()\n",
       "        )\n",
       "        (sa): SelfAttention(\n",
       "          (query): Conv1d(42, 5, kernel_size=(1,), stride=(1,), bias=False)\n",
       "          (key): Conv1d(42, 5, kernel_size=(1,), stride=(1,), bias=False)\n",
       "          (value): Conv1d(42, 42, kernel_size=(1,), stride=(1,), bias=False)\n",
       "        )\n",
       "        (fc_fusion): Linear(in_features=210, out_features=84, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (shared_encoder): SharedEncoder(\n",
       "      (conv_layers): Sequential(\n",
       "        (0): Conv2d(1, 42, kernel_size=(5, 1), stride=(2, 1))\n",
       "        (1): ReLU()\n",
       "        (2): Conv2d(42, 42, kernel_size=(5, 1), stride=(2, 1))\n",
       "        (3): ReLU()\n",
       "        (4): Conv2d(42, 42, kernel_size=(5, 1), stride=(2, 1))\n",
       "        (5): ReLU()\n",
       "        (6): Conv2d(42, 42, kernel_size=(5, 1), stride=(2, 1))\n",
       "        (7): ReLU()\n",
       "      )\n",
       "      (sa): SelfAttention(\n",
       "        (query): Conv1d(42, 5, kernel_size=(1,), stride=(1,), bias=False)\n",
       "        (key): Conv1d(42, 5, kernel_size=(1,), stride=(1,), bias=False)\n",
       "        (value): Conv1d(42, 42, kernel_size=(1,), stride=(1,), bias=False)\n",
       "      )\n",
       "      (fc_fusion): Linear(in_features=210, out_features=84, bias=True)\n",
       "    )\n",
       "    (residual_block): ResidualBlock(\n",
       "      (fc_proj): Linear(in_features=168, out_features=84, bias=True)\n",
       "    )\n",
       "    (missing_modality_feature_generation): MissingModalityFeatureGeneration()\n",
       "    (decoder): Decoder(\n",
       "      (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "      (fc_layer): Linear(in_features=3780, out_features=19, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ----------------------- load all the data -------------------\n",
      "-----------------------Sliding file are generated -----------------------\n",
      "-----------------------Sliding file are generated -----------------------\n",
      "================ LOCV Mode ====================\n",
      "================ 8 CV ======================\n",
      "================ the 0 th CV Experiment ================ \n",
      "Leave one Out Experiment : The 1 Part as the test\n",
      "[-] Target sampling weights:  [0.00263158 0.00265957 0.00266667 0.00269542 0.00257732 0.00265252\n",
      " 0.00271739 0.00263852 0.0026178  0.00263852 0.00265252 0.00265957\n",
      " 0.00271003 0.0027027  0.0025974  0.00261097 0.00260417 0.00261097\n",
      " 0.00263158]\n",
      "Train data number :  7182\n",
      "The number of classes is :  19\n",
      "The input_length  is :  125\n",
      "The channel_in is :  45\n",
      "Validation data number :  798\n",
      "Test data number :  1140\n",
      "SKIP FINE TUNING?  False\n",
      "================ Build the model ================ \n",
      "Build the ShaSpec model!\n",
      "-----------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Process [epochs]:   0%|          | 0/60 [01:15<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mexp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Bachelorarbeit/I2S0W2C2_CFC/notebooks/model/../../experiment.py:398\u001b[0m, in \u001b[0;36mExp.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    395\u001b[0m epoch_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m    397\u001b[0m pbar \u001b[38;5;241m=\u001b[39m tqdm(train_loader, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mtrain_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m, leave\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m--> 398\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m (batch_x1,_,batch_y) \u001b[38;5;129;01min\u001b[39;00m train_loader:                        \n\u001b[1;32m    399\u001b[0m     batch_x1 \u001b[38;5;241m=\u001b[39m batch_x1\u001b[38;5;241m.\u001b[39mdouble()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice) \u001b[38;5;66;03m#--\u001b[39;00m\n\u001b[1;32m    401\u001b[0m     \u001b[38;5;66;03m# ShaSpec model takes modalities as input\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/Bachelorarbeit/I2S0W2C2_CFC/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/Documents/Bachelorarbeit/I2S0W2C2_CFC/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:674\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    672\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    673\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 674\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    675\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    676\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "exp.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
